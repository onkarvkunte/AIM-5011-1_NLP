{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\sayed\\anaconda\\envs\\torch2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern3 = r'P+'  # Define the regex pattern to match consecutive 'P's\n",
    "def process3(x):\n",
    "    p = re.sub(pattern3, \"\", x)  # Use re.sub to replace the pattern with an empty string\n",
    "    return p  # Return the processed string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the process1 function to each element in the target_text column\n",
    "new.target_text = new.target_text.apply(lambda x: process1(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2796 west golden willow drive</td>\n",
       "      <td>189 steth werere re roure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9734719887</td>\n",
       "      <td>aranere-199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4977236992</td>\n",
       "      <td>989-111-6662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reallyloud.co.uksimaii</td>\n",
       "      <td>delen rereron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kkaicd1.pixnet.net</td>\n",
       "      <td>arisalallange</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          target                 prediction\n",
       "0  2796 west golden willow drive  189 steth werere re roure\n",
       "1                     9734719887                aranere-199\n",
       "2                     4977236992               989-111-6662\n",
       "3         reallyloud.co.uksimaii              delen rereron\n",
       "4             kkaicd1.pixnet.net              arisalallange"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_tf_utils import get_initializer\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Set the number of threads for intra-op and inter-op parallelism\n",
    "tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "\n",
    "# Check if a pretrained model exists, otherwise download it\n",
    "if os.path.exists(\"tf_gpt2_keras_lora\"):\n",
    "    print(\"Model exists\")\n",
    "    # Use the pretrained model if it exists\n",
    "    model = TFGPT2LMHeadModel.from_pretrained(\"tf_gpt2_keras_lora\")\n",
    "else:\n",
    "    print(\"Downloading model\")\n",
    "    # Download and use the gpt2 pretrained model\n",
    "    model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:/data/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1795 westh st rerorore</td>\n",
       "      <td>2796 west golden willow drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+39-313-5199</td>\n",
       "      <td>9734719887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>998-121-1662</td>\n",
       "      <td>4977236992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elle cororerin</td>\n",
       "      <td>reallyloud.co.uksimaii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arilalallanges</td>\n",
       "      <td>kkaicd1.pixnet.net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               prediction                    target_text\n",
       "0  1795 westh st rerorore  2796 west golden willow drive\n",
       "1            +39-313-5199                     9734719887\n",
       "2            998-121-1662                     4977236992\n",
       "3          elle cororerin         reallyloud.co.uksimaii\n",
       "4          arilalallanges             kkaicd1.pixnet.net"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is the collective dataset containing predictions and targets\n",
    "# Replace 'collective' with the correct path and file name if needed\n",
    "\n",
    "# Extract the 'prediction' column values as a list\n",
    "inputs = data[\"prediction\"].tolist()\n",
    "\n",
    "# Extract the 'target' column values as a list\n",
    "targets = data[\"target\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#import distance \n",
    "\n",
    "# Initialize empty lists to store input and target encodings\n",
    "input_ids = []\n",
    "target_ids = []\n",
    "\n",
    "# Loop through inputs and targets\n",
    "for inp, tgt in zip(inputs, targets):\n",
    "    # Encode input and target sequences using the tokenizer\n",
    "    input_encoding = tokenizer.encode(inp, add_special_tokens=True, max_length=1024, truncation=True)\n",
    "    target_encoding = tokenizer.encode(tgt, add_special_tokens=True, max_length=1024, truncation=True)\n",
    "    \n",
    "    # Append encoded input and target sequences to lists\n",
    "    input_ids.append(input_encoding)\n",
    "    target_ids.append(target_encoding)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "input_train, input_test, target_train, target_test = train_test_split(input_ids, target_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define some parameters for training\n",
    "batch_size = 2\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Define the optimizer and loss function for the model training\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Levenshtein distance-based accuracy\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    accuracies = []  # Create an empty list to store individual accuracies\n",
    "    for pred, target in zip(predictions, targets):  # Loop through predicted and target sequences\n",
    "        pred_text = tokenizer.decode(pred, skip_special_tokens=True)  # Convert predicted token IDs to text\n",
    "        target_text = tokenizer.decode(target, skip_special_tokens=True)  # Convert target token IDs to text\n",
    "        # Calculate Levenshtein distance-based accuracy for this pair of sequences\n",
    "        accuracy = 1 - (distance.levenshtein(pred_text, target_text) / max(len(pred_text), len(target_text)))\n",
    "        accuracies.append(accuracy)  # Add accuracy to the list\n",
    "    return np.mean(accuracies)  # Calculate and return the mean accuracy across all pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Batch 0/512 - loss: 9.0748\n",
      "Batch 20/512 - loss: 6.6302\n",
      "Batch 40/512 - loss: 7.4509\n",
      "Batch 60/512 - loss: 7.2791\n",
      "Batch 80/512 - loss: 5.7221\n",
      "Batch 100/512 - loss: 5.0603\n",
      "Batch 120/512 - loss: 4.6360\n",
      "Batch 140/512 - loss: 3.9326\n",
      "Batch 160/512 - loss: 6.1096\n",
      "Batch 180/512 - loss: 2.7365\n",
      "Batch 200/512 - loss: 2.8067\n",
      "Batch 220/512 - loss: 4.6523\n",
      "Batch 240/512 - loss: 2.2090\n",
      "Batch 260/512 - loss: 5.9496\n",
      "Batch 280/512 - loss: 4.2090\n",
      "Batch 300/512 - loss: 5.5323\n",
      "Batch 320/512 - loss: 4.2686\n",
      "Batch 340/512 - loss: 3.3355\n",
      "Batch 360/512 - loss: 4.0083\n",
      "Batch 380/512 - loss: 3.0165\n",
      "Batch 400/512 - loss: 1.6922\n",
      "Batch 420/512 - loss: 2.5310\n",
      "Batch 440/512 - loss: 3.7592\n",
      "Batch 460/512 - loss: 3.4074\n",
      "Batch 480/512 - loss: 1.6323\n",
      "Batch 500/512 - loss: 0.4391\n",
      "Epoch 2/3\n",
      "Batch 0/512 - loss: 4.8635\n",
      "Batch 20/512 - loss: 4.7457\n",
      "Batch 40/512 - loss: 5.2103\n",
      "Batch 60/512 - loss: 5.2140\n",
      "Batch 80/512 - loss: 3.7198\n",
      "Batch 100/512 - loss: 2.9074\n",
      "Batch 120/512 - loss: 2.9727\n",
      "Batch 140/512 - loss: 1.5464\n",
      "Batch 160/512 - loss: 2.5272\n",
      "Batch 180/512 - loss: 1.3784\n",
      "Batch 200/512 - loss: 2.0977\n",
      "Batch 220/512 - loss: 3.5702\n",
      "Batch 240/512 - loss: 0.8278\n",
      "Batch 260/512 - loss: 4.0410\n",
      "Batch 280/512 - loss: 2.6160\n",
      "Batch 300/512 - loss: 4.1324\n",
      "Batch 320/512 - loss: 3.0290\n",
      "Batch 340/512 - loss: 1.7958\n",
      "Batch 360/512 - loss: 3.0606\n",
      "Batch 380/512 - loss: 1.9774\n",
      "Batch 400/512 - loss: 1.3629\n",
      "Batch 420/512 - loss: 1.0950\n",
      "Batch 440/512 - loss: 2.8791\n",
      "Batch 460/512 - loss: 1.7456\n",
      "Batch 480/512 - loss: 1.2957\n",
      "Batch 500/512 - loss: 0.0660\n",
      "Epoch 3/3\n",
      "Batch 0/512 - loss: 2.4392\n",
      "Batch 20/512 - loss: 2.9743\n",
      "Batch 40/512 - loss: 4.1255\n",
      "Batch 60/512 - loss: 3.6529\n",
      "Batch 80/512 - loss: 2.6738\n",
      "Batch 100/512 - loss: 1.0927\n",
      "Batch 120/512 - loss: 1.9557\n",
      "Batch 140/512 - loss: 0.5705\n",
      "Batch 160/512 - loss: 0.2072\n",
      "Batch 180/512 - loss: 0.9207\n",
      "Batch 200/512 - loss: 1.9109\n",
      "Batch 220/512 - loss: 2.6748\n",
      "Batch 240/512 - loss: 0.9794\n",
      "Batch 260/512 - loss: 3.0012\n",
      "Batch 280/512 - loss: 1.9332\n",
      "Batch 300/512 - loss: 3.4431\n",
      "Batch 320/512 - loss: 2.1374\n",
      "Batch 340/512 - loss: 0.8527\n",
      "Batch 360/512 - loss: 1.9605\n",
      "Batch 380/512 - loss: 1.3027\n",
      "Batch 400/512 - loss: 0.4292\n",
      "Batch 420/512 - loss: 0.9907\n",
      "Batch 440/512 - loss: 1.1628\n",
      "Batch 460/512 - loss: 0.8736\n",
      "Batch 480/512 - loss: 0.7862\n",
      "Batch 500/512 - loss: 0.0176\n"
     ]
    }
   ],
   "source": [
    "# Define some params\n",
    "batch_size = 2\n",
    "num_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for i in range(0, len(input_ids), batch_size):\n",
    "        batch_inputs = input_ids[i:i+batch_size]\n",
    "        batch_targets = target_ids[i:i+batch_size]\n",
    "\n",
    "        # Find the maximum sequence length in the batch\n",
    "        max_length = max(max(len(seq) for seq in batch_inputs), max(len(seq) for seq in batch_targets)\n",
    "        \n",
    "        # Pad the input and target sequences to the same maximum length\n",
    "        batch_inputs = tf.keras.preprocessing.sequence.pad_sequences(batch_inputs, padding=\"post\", maxlen=max_length)\n",
    "        batch_targets = tf.keras.preprocessing.sequence.pad_sequences(batch_targets, padding=\"post\", maxlen=max_length)\n",
    "\n",
    "        # Compute the predictions and loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(batch_inputs)[0]\n",
    "            loss = loss_fn(batch_targets, logits)\n",
    "\n",
    "        # Compute the gradients and update the parameters\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # Print the loss every 10 batches\n",
    "        if i % (10 * batch_size) == 0:\n",
    "            print(f\"Batch {i}/{len(input_ids)} - loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: are you donet comonn\n",
      "Generated text: are you donet comonn!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# Example of generating text using the loaded model\n",
    "# Input text to generate from\n",
    "input_text = \"are you donet comonn\"\n",
    "\n",
    "# Encode the input text using the tokenizer\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True, max_length=1024, truncation=True)\n",
    "\n",
    "# Generate text with the loaded model\n",
    "output = model.generate(\n",
    "    tf.constant([input_ids]),  # Provide the encoded input_ids\n",
    "    max_length=100,             # Maximum length of the generated text\n",
    "    do_sample=True,             # Allow for sampling\n",
    "    top_k=50,                   # Top-k tokens to consider during sampling\n",
    "    top_p=0.95,                 # Top-p (nucleus) sampling\n",
    "    temperature=0.9,            # Sampling temperature\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id to eos_token_id\n",
    "    attention_mask=tf.constant([[1] * len(input_ids)]),  # Pass attention_mask\n",
    ")\n",
    "\n",
    "# Decode the generated output to text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the input and generated text\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated text: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "input_train, input_test, target_train, target_test = train_test_split(input_ids, target_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to calculate Levenshtein distance-based accuracy\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    accuracies = []\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_text = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "        target_text = tokenizer.decode(target, skip_special_tokens=True)\n",
    "        accuracy = 1 - (distance.levenshtein(pred_text, target_text) / max(len(pred_text), len(target_text)))\n",
    "        accuracies.append(accuracy)\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "# Generate predictions for the test set\n",
    "test_predictions = loaded_model.generate(\n",
    "    tf.constant(input_test),\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    attention_mask=tf.constant([[1] * len(seq) for seq in input_test])\n",
    ")\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = calculate_accuracy(test_predictions, target_test)\n",
    "print(\"Levenshtein Distance-based Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
