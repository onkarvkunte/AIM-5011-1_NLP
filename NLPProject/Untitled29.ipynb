{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd95ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input,GRU,LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "import six\n",
    "from tensorflow.keras.utils import deserialize_keras_object\n",
    "from joblib import dump, load\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input,GRU,Embedding,Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Activation, Dropout, GRU, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    " \n",
    "from tensorflow.keras.layers import concatenate, Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd1e2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    Encoder model -- That takes a input sequence and returns output sequence\n",
    "    '''\n",
    "\n",
    "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
    "\n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        \n",
    "      super().__init__()\n",
    "\n",
    "      self.e_embed = Embedding(input_dim=inp_vocab_size, output_dim=embedding_size, input_length= input_length)\n",
    "      self.lstm_size = lstm_size\n",
    "      \n",
    "      self.e_lstm = LSTM(lstm_size, return_sequences=True, return_state=True)\n",
    "\n",
    "\n",
    "    def call(self,input_sequence,states):\n",
    "        '''\n",
    "          This function takes a sequence input and the initial states of the encoder.\n",
    "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
    "          returns -- encoder_output, last time step's hidden and cell state\n",
    "        '''\n",
    "        embedding = self.e_embed(input_sequence)\n",
    "        # print(embedding.shape)\n",
    "        # output_state, enc_h, enc_c = self.e_lstm( embedding, initial_state = states)\n",
    "        output_state, enc_h, enc_c = self.e_lstm( embedding)\n",
    "\n",
    "        return output_state, enc_h, enc_c\n",
    "\n",
    "      \n",
    "\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "      '''\n",
    "      Given a batch size it will return intial hidden state and intial cell state.\n",
    "      If batch size is 32- Hidden state is zeros of size [32,lstm_units], cell state zeros is of size [32,lstm_units]\n",
    "      '''\n",
    "      return [tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3931f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Class the calculates score based on the scoring_function using Bahdanu attention mechanism.\n",
    "  '''\n",
    "  def __init__(self,scoring_function, att_units):\n",
    "\n",
    "    super(Attention, self).__init__()\n",
    "\n",
    "    self.scoring_function = scoring_function\n",
    "\n",
    "\n",
    "    # Please go through the reference notebook and research paper to complete the scoring functions\n",
    "\n",
    "    if self.scoring_function=='dot':\n",
    "      # Intialize variables needed for Dot score function here\n",
    "      self.dot= tf.keras.layers.Dot(axes = (1,2))      \n",
    "      pass\n",
    "    elif scoring_function == 'general':\n",
    "      # Intialize variables needed for General score function here\n",
    "      self.w= Dense(att_units)\n",
    "      self.dot = tf.keras.layers.Dot(axes = (1,2))\n",
    "      pass\n",
    "    elif scoring_function == 'concat':\n",
    "      # Intialize variables needed for Concat score function here\n",
    "      self.w1= Dense(att_units)\n",
    "      self.w2= Dense(att_units)\n",
    "      self.v= Dense(1)\n",
    "      pass\n",
    "  \n",
    "  \n",
    "  def call(self,decoder_hidden_state,encoder_output):\n",
    "    '''\n",
    "      Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
    "      * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
    "        Multiply the score function with your encoder_outputs to get the context vector.\n",
    "        Function returns context vector and attention weights(softmax - scores)\n",
    "    '''\n",
    "    decoder_hidden_state = tf.expand_dims(decoder_hidden_state, 1)\n",
    "    if self.scoring_function == 'dot':\n",
    "        # Implement Dot score function here\n",
    "        score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0,2,1)), encoder_output]), (0,2,1))\n",
    "        # print(\"In Dot\")\n",
    "\n",
    "        pass\n",
    "    elif self.scoring_function == 'general':\n",
    "        # Implement General score function here\n",
    "        # print(encoder_output.shape)\n",
    "        mulpy = self.w(encoder_output)\n",
    "        # print(mulpy.shape)\n",
    "        score = tf.transpose(self.dot([tf.transpose(decoder_hidden_state, (0, 2, 1)), mulpy]), (0, 2,1),)\n",
    "\n",
    "        pass\n",
    "    elif self.scoring_function == 'concat':\n",
    "        inte = self.w1(decoder_hidden_state) + self.w2(encoder_output)\n",
    "        tan = tf.nn.tanh(inte)\n",
    "        score = self.v(tan)\n",
    "\n",
    "    att_weights = tf.nn.softmax(score, axis =1)\n",
    "    context_vector = att_weights * encoder_output\n",
    "    context_vector = tf.reduce_sum( context_vector, axis=1)\n",
    "    return context_vector, att_weights\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7301ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepDecoder(tf.keras.Model):\n",
    "  def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "\n",
    "      # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "      super(OneStepDecoder, self).__init__()\n",
    "      self.dec_embed = Embedding(input_dim = tar_vocab_size, output_dim = embedding_dim)\n",
    "      self.lstm = LSTM(dec_units, return_sequences = True, return_state = True)\n",
    "      self.attention = Attention(scoring_function = score_fun, att_units = att_units)\n",
    "      self.den = Dense(tar_vocab_size)\n",
    "\n",
    "\n",
    "  def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
    "    '''\n",
    "        One step decoder mechanisim step by step:\n",
    "      A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "      B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "      C. Concat the context vector with the step A output\n",
    "      D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "      E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "      F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "    '''\n",
    "    embd = self.dec_embed(input_to_decoder)\n",
    "    context_vec, attention_weights = self.attention( state_h,  encoder_output)    \n",
    "\n",
    "    f_inp = tf.concat([tf.expand_dims(context_vec, 1), embd], axis = -1)\n",
    "    # print(f_inp.shape)\n",
    "    out, dec_h, dec_c = self.lstm(f_inp, [state_h, state_c])\n",
    "    out = tf.reshape( out, (-1, out.shape[2]))\n",
    "    output = self.den(out)\n",
    "\n",
    "    return output, dec_h, dec_c, attention_weights, context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7cbffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
    "      #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "      super(Decoder, self).__init__()\n",
    "      self.input_length = input_length\n",
    "      self.out_vocab_size = out_vocab_size\n",
    "      self.oneStepDecoder = OneStepDecoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
    "      self.out_vocab_size = out_vocab_size\n",
    "        \n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
    "\n",
    "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
    "        #Create a tensor array as shown in the reference notebook\n",
    "        \n",
    "        #Iterate till the length of the decoder input\n",
    "            # Call onestepdecoder for each token in decoder_input\n",
    "            # Store the output in tensorarray\n",
    "        # Return the tensor array\n",
    "        # outputs = tf.TensorArray(dtype =  tf.float32, size= input_to_decoder.shape[1])\n",
    "        outputs = tf.TensorArray(dtype =  tf.float32, size= tf.shape(input_to_decoder)[1])\n",
    "\n",
    "        \n",
    "        for timestep in range(tf.shape(input_to_decoder)[1]):\n",
    "\n",
    "            output, decoder_hidden_state, decoder_cell_state, _, _ = self.oneStepDecoder(input_to_decoder[:, timestep:timestep+1],\n",
    "                                                                                          encoder_output,decoder_hidden_state,decoder_cell_state)                                                                                            \n",
    "                                                                                             \n",
    "                                                                                             \n",
    "            # Store the output in tensorarray\n",
    "            outputs = outputs.write(timestep, output)\n",
    "        # Return the tensor array\n",
    "        outputs = tf.transpose(outputs.stack(), (1, 0, 2))\n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "138068e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "  def __init__(self,inp_vocab_size, out_vocab_size,input_length, enc_units,embedding_dim, dec_units, max_len, score_fun, att_units, batch_size):\n",
    "    #Intialize objects from encoder decoder\n",
    "    super(encoder_decoder, self).__init__()\n",
    "    self.encoder = Encoder(inp_vocab_size= inp_vocab_size +1, embedding_size=embedding_dim, lstm_size= att_units,input_length= max_len)\n",
    "    self.decoder = Decoder(out_vocab_size +1, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  \n",
    "  def call(self,data):\n",
    "    #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
    "    # Decoder initial states are encoder final states, Initialize it accordingly\n",
    "    # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
    "    # return the decoder output\n",
    "    e_inp, d_inp = data[0], data[1]\n",
    "    # print(data[0].shape)\n",
    "    # print(data[1].shape)\n",
    "\n",
    "\n",
    "    initial_state = self.encoder.initialize_states(self.batch_size)\n",
    "\n",
    "    e_output, enc_h, enc_c = self.encoder(e_inp,initial_state)\n",
    "    outputs = tf.TensorArray(dtype = tf.float32, size= 20)\n",
    "        \n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "    # print(dec_h.shape)\n",
    "    output=self.decoder(d_inp,e_output, dec_h, dec_c)\n",
    "  \n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c20065e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e3c9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/sayedraheel/Desktop/NLP/Sentence_correction/tokenizer.word_index.json', 'r') as json_file:\n",
    "    loaded_word_index = json.load(json_file)\n",
    "\n",
    "# Create a new tokenizer and set its word_index to the loaded word_index\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.word_index = loaded_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0a46272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/sayedraheel/Desktop/NLP/Sentence_correction/tokenizer_e.word_index.json', 'r') as json_file:\n",
    "    loaded_word_index1 = json.load(json_file)\n",
    "\n",
    "# Create a new tokenizer and set its word_index to the loaded word_index\n",
    "tokenizer_e = Tokenizer()\n",
    "tokenizer_e.word_index = loaded_word_index1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "559af87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7815\n",
      "5464\n"
     ]
    }
   ],
   "source": [
    "encoder_vocab_size=len(tokenizer.word_index.keys())\n",
    "print(encoder_vocab_size)\n",
    "decoder_vocab_size=len(tokenizer_e.word_index.keys())\n",
    "print(decoder_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc94dacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<start>': 1,\n",
       " '<end>': 2,\n",
       " 'I': 3,\n",
       " 'you': 4,\n",
       " 'to': 5,\n",
       " 'the': 6,\n",
       " 'are': 7,\n",
       " 'is': 8,\n",
       " 'a': 9,\n",
       " 'for': 10,\n",
       " 'and': 11,\n",
       " 'at': 12,\n",
       " \"I'm\": 13,\n",
       " 'your': 14,\n",
       " 'my': 15,\n",
       " 'You': 16,\n",
       " 'go': 17,\n",
       " 'have': 18,\n",
       " 'me': 19,\n",
       " 'in': 20,\n",
       " 'of': 21,\n",
       " 'can': 22,\n",
       " 'be': 23,\n",
       " 'want': 24,\n",
       " 'on': 25,\n",
       " 'not': 26,\n",
       " 'will': 27,\n",
       " \"don't\": 28,\n",
       " 'with': 29,\n",
       " 'or': 30,\n",
       " 'So': 31,\n",
       " 'it': 32,\n",
       " 'am': 33,\n",
       " 'going': 34,\n",
       " 'do': 35,\n",
       " 'Haha.': 36,\n",
       " 'that': 37,\n",
       " 'so': 38,\n",
       " 'got': 39,\n",
       " 'just': 40,\n",
       " 'then': 41,\n",
       " 'think': 42,\n",
       " 'Then': 43,\n",
       " 'know': 44,\n",
       " 'Hey,': 45,\n",
       " 'you.': 46,\n",
       " 'we': 47,\n",
       " 'What': 48,\n",
       " 'But': 49,\n",
       " 'if': 50,\n",
       " 'time': 51,\n",
       " 'meet': 52,\n",
       " 'what': 53,\n",
       " 'very': 54,\n",
       " 'already.': 55,\n",
       " 'How': 56,\n",
       " 'this': 57,\n",
       " \"I'll\": 58,\n",
       " 'see': 59,\n",
       " 'you?': 60,\n",
       " 'still': 61,\n",
       " 'all': 62,\n",
       " 'come': 63,\n",
       " 'Are': 64,\n",
       " 'about': 65,\n",
       " 'like': 66,\n",
       " 'get': 67,\n",
       " 'from': 68,\n",
       " 'now.': 69,\n",
       " 'up': 70,\n",
       " 'out': 71,\n",
       " 'need': 72,\n",
       " 'but': 73,\n",
       " 'Can': 74,\n",
       " 'me.': 75,\n",
       " 'Haha,': 76,\n",
       " 'was': 77,\n",
       " 'when': 78,\n",
       " 'how': 79,\n",
       " 'Ok.': 80,\n",
       " 'Yes.': 81,\n",
       " 'My': 82,\n",
       " 'back': 83,\n",
       " 'one': 84,\n",
       " 'call': 85,\n",
       " 'Do': 86,\n",
       " \"It's\": 87,\n",
       " 'right?': 88,\n",
       " 'We': 89,\n",
       " \"Don't\": 90,\n",
       " 'because': 91,\n",
       " 'home': 92,\n",
       " 'she': 93,\n",
       " 'If': 94,\n",
       " 'no': 95,\n",
       " 'around': 96,\n",
       " 'good': 97,\n",
       " 'never': 98,\n",
       " 'there': 99,\n",
       " \"it's\": 100,\n",
       " 'they': 101,\n",
       " 'also': 102,\n",
       " 'too': 103,\n",
       " 'Hey': 104,\n",
       " 'Hi,': 105,\n",
       " 'where': 106,\n",
       " 'ask': 107,\n",
       " 'take': 108,\n",
       " 'Hi': 109,\n",
       " 'more': 110,\n",
       " 'free': 111,\n",
       " 'as': 112,\n",
       " 'did': 113,\n",
       " 'The': 114,\n",
       " 'after': 115,\n",
       " 'Just': 116,\n",
       " 'an': 117,\n",
       " 'tomorrow': 118,\n",
       " 'next': 119,\n",
       " 'buy': 120,\n",
       " 'message': 121,\n",
       " 'Because': 122,\n",
       " 'Yes,': 123,\n",
       " 'should': 124,\n",
       " 'her': 125,\n",
       " 'See': 126,\n",
       " 'only': 127,\n",
       " 'now?': 128,\n",
       " 'Ok,': 129,\n",
       " 'time.': 130,\n",
       " 'Is': 131,\n",
       " 'And': 132,\n",
       " 'must': 133,\n",
       " 'it.': 134,\n",
       " 'coming': 135,\n",
       " 'Where': 136,\n",
       " 'it?': 137,\n",
       " 'It': 138,\n",
       " 'by': 139,\n",
       " 'some': 140,\n",
       " 'doing': 141,\n",
       " \"didn't\": 142,\n",
       " 'really': 143,\n",
       " 'bring': 144,\n",
       " 'now': 145,\n",
       " 'Not': 146,\n",
       " 'having': 147,\n",
       " 'make': 148,\n",
       " 'day': 149,\n",
       " 'not?': 150,\n",
       " 'too.': 151,\n",
       " 'No': 152,\n",
       " 'find': 153,\n",
       " \"haven't\": 154,\n",
       " 'Good': 155,\n",
       " 'also.': 156,\n",
       " 'quite': 157,\n",
       " 'Oh,': 158,\n",
       " 'tomorrow.': 159,\n",
       " 'Hehe.': 160,\n",
       " 'Why': 161,\n",
       " 'No.': 162,\n",
       " 'Have': 163,\n",
       " 'he': 164,\n",
       " 'Tomorrow': 165,\n",
       " 'long': 166,\n",
       " 'been': 167,\n",
       " 'late': 168,\n",
       " 'then.': 169,\n",
       " 'know.': 170,\n",
       " 'late.': 171,\n",
       " 'tomorrow?': 172,\n",
       " 'Hope': 173,\n",
       " 'Okay.': 174,\n",
       " \"can't\": 175,\n",
       " 'Hey.': 176,\n",
       " 'thought': 177,\n",
       " 'ok?': 178,\n",
       " 'tell': 179,\n",
       " 'our': 180,\n",
       " 'working': 181,\n",
       " 'care': 182,\n",
       " 'Oh': 183,\n",
       " 'already?': 184,\n",
       " 'lecture': 185,\n",
       " 'any': 186,\n",
       " 'He': 187,\n",
       " 'new': 188,\n",
       " 'wait': 189,\n",
       " 'Call': 190,\n",
       " 'friend': 191,\n",
       " 'look': 192,\n",
       " '2': 193,\n",
       " 'help': 194,\n",
       " 'why': 195,\n",
       " 'before': 196,\n",
       " 'had': 197,\n",
       " 'OK.': 198,\n",
       " 'Maybe': 199,\n",
       " 'home.': 200,\n",
       " \"won't\": 201,\n",
       " 'first.': 202,\n",
       " 'than': 203,\n",
       " 'see.': 204,\n",
       " 'today.': 205,\n",
       " 'later': 206,\n",
       " 'east': 207,\n",
       " 'Did': 208,\n",
       " 'here.': 209,\n",
       " \"I've\": 210,\n",
       " 'one.': 211,\n",
       " 'At': 212,\n",
       " 'Or': 213,\n",
       " 'way': 214,\n",
       " 'say': 215,\n",
       " 'driving': 216,\n",
       " 'you,': 217,\n",
       " 'house': 218,\n",
       " 'were': 219,\n",
       " 'school': 220,\n",
       " 'us': 221,\n",
       " 'send': 222,\n",
       " 'eat': 223,\n",
       " 'something': 224,\n",
       " 'number': 225,\n",
       " 'dinner': 226,\n",
       " 'Take': 227,\n",
       " 'feel': 228,\n",
       " 'people': 229,\n",
       " 'today': 230,\n",
       " 'nice': 231,\n",
       " 'yet.': 232,\n",
       " 'enjoy': 233,\n",
       " 'work': 234,\n",
       " 'first': 235,\n",
       " 'night.': 236,\n",
       " 'When': 237,\n",
       " 'watch': 238,\n",
       " 'has': 239,\n",
       " 'me?': 240,\n",
       " 'Thanks.': 241,\n",
       " 'went': 242,\n",
       " 'lunch': 243,\n",
       " 'not.': 244,\n",
       " 'way,': 245,\n",
       " 'early': 246,\n",
       " 'maybe': 247,\n",
       " 'who': 248,\n",
       " 'time?': 249,\n",
       " 'today?': 250,\n",
       " 'okay?': 251,\n",
       " 'much': 252,\n",
       " 'Oh.': 253,\n",
       " 'introduce': 254,\n",
       " 'always': 255,\n",
       " 'chat?': 256,\n",
       " 'last': 257,\n",
       " 'cannot': 258,\n",
       " 'outside': 259,\n",
       " 'yet?': 260,\n",
       " 'phone': 261,\n",
       " 'bus': 262,\n",
       " 'later.': 263,\n",
       " 'place': 264,\n",
       " 'dinner.': 265,\n",
       " 'Okay,': 266,\n",
       " 'stay': 267,\n",
       " 'Got': 268,\n",
       " 'sister': 269,\n",
       " 'book': 270,\n",
       " 'soon.': 271,\n",
       " 'They': 272,\n",
       " 'give': 273,\n",
       " 'friends': 274,\n",
       " 'love': 275,\n",
       " 'nothing': 276,\n",
       " 'wants': 277,\n",
       " 'ok.': 278,\n",
       " 'She': 279,\n",
       " 'chat': 280,\n",
       " 'bought': 281,\n",
       " 'Go': 282,\n",
       " 'cut': 283,\n",
       " 'i': 284,\n",
       " \"That's\": 285,\n",
       " 'few': 286,\n",
       " 'already': 287,\n",
       " 'mind.': 288,\n",
       " 'out.': 289,\n",
       " 'Please': 290,\n",
       " 'hope': 291,\n",
       " 'Your': 292,\n",
       " 'bit': 293,\n",
       " 'Anyway,': 294,\n",
       " 'Sorry.': 295,\n",
       " 'Sigh.': 296,\n",
       " 'night': 297,\n",
       " 'there?': 298,\n",
       " 'drive': 299,\n",
       " 'again': 300,\n",
       " 'might': 301,\n",
       " '12': 302,\n",
       " 'down': 303,\n",
       " 'meeting': 304,\n",
       " 'show': 305,\n",
       " 'sure': 306,\n",
       " 'saw': 307,\n",
       " 'better': 308,\n",
       " 'reach': 309,\n",
       " 'lots': 310,\n",
       " 'birthday': 311,\n",
       " 'me,': 312,\n",
       " 'No,': 313,\n",
       " 'number.': 314,\n",
       " '3': 315,\n",
       " 'end': 316,\n",
       " 'By': 317,\n",
       " 'Want': 318,\n",
       " 'going?': 319,\n",
       " 'you!': 320,\n",
       " 'enough': 321,\n",
       " 'lot': 322,\n",
       " 'Friday': 323,\n",
       " 'off': 324,\n",
       " 'job': 325,\n",
       " 'another': 326,\n",
       " 'that?': 327,\n",
       " 'Never': 328,\n",
       " 'Happy': 329,\n",
       " 'anything': 330,\n",
       " 'other': 331,\n",
       " 'please.': 332,\n",
       " 'again.': 333,\n",
       " 'Thursday': 334,\n",
       " 'change': 335,\n",
       " 'handphone': 336,\n",
       " 'reply': 337,\n",
       " 'course': 338,\n",
       " 'good.': 339,\n",
       " 'only.': 340,\n",
       " 'That': 341,\n",
       " 'many': 342,\n",
       " 'going.': 343,\n",
       " 'doing?': 344,\n",
       " 'can.': 345,\n",
       " 'day.': 346,\n",
       " 'what?': 347,\n",
       " 'Hi!': 348,\n",
       " 'great': 349,\n",
       " 'which': 350,\n",
       " 'Orchard': 351,\n",
       " 'Hee.': 352,\n",
       " 'life': 353,\n",
       " 'let': 354,\n",
       " 'talking': 355,\n",
       " 'says': 356,\n",
       " 'that.': 357,\n",
       " 'mind': 358,\n",
       " 'put': 359,\n",
       " 'said': 360,\n",
       " 'go.': 361,\n",
       " 'sweet': 362,\n",
       " 'month': 363,\n",
       " 'pass': 364,\n",
       " 'hard': 365,\n",
       " 'mean': 366,\n",
       " 'number?': 367,\n",
       " 'studying': 368,\n",
       " 'up.': 369,\n",
       " '1': 370,\n",
       " 'You?': 371,\n",
       " 'sleep': 372,\n",
       " 'Help': 373,\n",
       " 'play': 374,\n",
       " 'told': 375,\n",
       " 'later?': 376,\n",
       " '6': 377,\n",
       " 'use': 378,\n",
       " 'OK?': 379,\n",
       " \"What's\": 380,\n",
       " 'now,': 381,\n",
       " 'soon': 382,\n",
       " 'OK,': 383,\n",
       " 'watching': 384,\n",
       " 'join': 385,\n",
       " 'monkey': 386,\n",
       " 'would': 387,\n",
       " 'someone': 388,\n",
       " 'girl': 389,\n",
       " 'Hello,': 390,\n",
       " 'May': 391,\n",
       " 'her.': 392,\n",
       " 'called': 393,\n",
       " 'asked': 394,\n",
       " 'Monday': 395,\n",
       " 'there.': 396,\n",
       " 'until': 397,\n",
       " 'email': 398,\n",
       " 'tonight.': 399,\n",
       " 'study': 400,\n",
       " 'days': 401,\n",
       " 'big': 402,\n",
       " 'Still': 403,\n",
       " \"there's\": 404,\n",
       " 'week': 405,\n",
       " 'Meet': 406,\n",
       " 'Why?': 407,\n",
       " 'Going': 408,\n",
       " 'school.': 409,\n",
       " 'remember': 410,\n",
       " 'hair': 411,\n",
       " 'Of': 412,\n",
       " 'him': 413,\n",
       " 'please': 414,\n",
       " 'Hello': 415,\n",
       " 'buy.': 416,\n",
       " 'free.': 417,\n",
       " 'try': 418,\n",
       " 'shall': 419,\n",
       " 'same': 420,\n",
       " 'tonight?': 421,\n",
       " 'do.': 422,\n",
       " 'school?': 423,\n",
       " 'Thank': 424,\n",
       " 'Ah,': 425,\n",
       " 'busy': 426,\n",
       " 'back.': 427,\n",
       " 'sorry': 428,\n",
       " 'lesson.': 429,\n",
       " \"How's\": 430,\n",
       " 'money': 431,\n",
       " 'In': 432,\n",
       " 'talk': 433,\n",
       " 'reply.': 434,\n",
       " 'Chinese': 435,\n",
       " \"how's\": 436,\n",
       " 'into': 437,\n",
       " 'know,': 438,\n",
       " 'Later': 439,\n",
       " 'Yeah.': 440,\n",
       " 'driving.': 441,\n",
       " 'almost': 442,\n",
       " 'bad': 443,\n",
       " 'confirm': 444,\n",
       " 'rest': 445,\n",
       " 'NUS': 446,\n",
       " 'came': 447,\n",
       " 'week?': 448,\n",
       " 'them': 449,\n",
       " \"you're\": 450,\n",
       " 'Hello.': 451,\n",
       " 'finished': 452,\n",
       " 'Joey:': 453,\n",
       " 'Wow,': 454,\n",
       " 'near': 455,\n",
       " 'off.': 456,\n",
       " 'forget': 457,\n",
       " 'Anyway': 458,\n",
       " 'Hmm.': 459,\n",
       " 'okay.': 460,\n",
       " 'yourself.': 461,\n",
       " 'Today': 462,\n",
       " 'hand': 463,\n",
       " 'class': 464,\n",
       " 'thanks.': 465,\n",
       " 'getting': 466,\n",
       " 'SMS': 467,\n",
       " 'Ask': 468,\n",
       " 'those': 469,\n",
       " 'till': 470,\n",
       " 'ok': 471,\n",
       " 'wake': 472,\n",
       " 'Wow.': 473,\n",
       " 'people.': 474,\n",
       " 'way.': 475,\n",
       " 'doors': 476,\n",
       " 'locked': 477,\n",
       " 'strike': 478,\n",
       " 'Thanks': 479,\n",
       " 'guys': 480,\n",
       " 'come.': 481,\n",
       " 'eat.': 482,\n",
       " 'seen': 483,\n",
       " 'All': 484,\n",
       " 'Saturday.': 485,\n",
       " 'two': 486,\n",
       " 'thinking': 487,\n",
       " 'much.': 488,\n",
       " 'sounds': 489,\n",
       " 'work.': 490,\n",
       " 'finish': 491,\n",
       " 'here': 492,\n",
       " 'early.': 493,\n",
       " 'Now': 494,\n",
       " 'worry.': 495,\n",
       " 'Enjoy': 496,\n",
       " \"that's\": 497,\n",
       " \"what's\": 498,\n",
       " 'short': 499,\n",
       " 'anything.': 500,\n",
       " 'toss': 501,\n",
       " 'ball': 502,\n",
       " 'please?': 503,\n",
       " 'anyway.': 504,\n",
       " 'yard': 505,\n",
       " 'over': 506,\n",
       " 'care.': 507,\n",
       " 'well': 508,\n",
       " 'hall': 509,\n",
       " 'stuff': 510,\n",
       " 'how?': 511,\n",
       " 'computer': 512,\n",
       " 'sympathy': 513,\n",
       " 'Yup.': 514,\n",
       " 'Well,': 515,\n",
       " 'Monday.': 516,\n",
       " 'pick': 517,\n",
       " 'late,': 518,\n",
       " 'done': 519,\n",
       " 'A': 520,\n",
       " '.': 521,\n",
       " 'nation': 522,\n",
       " 'interested': 523,\n",
       " 'already,': 524,\n",
       " 'west': 525,\n",
       " 'dog': 526,\n",
       " 'walk': 527,\n",
       " 'Shuhui': 528,\n",
       " 'lovely': 529,\n",
       " 'wanted': 530,\n",
       " 'forgot': 531,\n",
       " 'through': 532,\n",
       " 'Really?': 533,\n",
       " 'lesson': 534,\n",
       " 'MRT': 535,\n",
       " 'Hi.': 536,\n",
       " 'Sorry,': 537,\n",
       " 'guess': 538,\n",
       " 'Yun,': 539,\n",
       " 'from?': 540,\n",
       " 'leave': 541,\n",
       " 'Will': 542,\n",
       " 'his': 543,\n",
       " 'win': 544,\n",
       " 'left': 545,\n",
       " '5': 546,\n",
       " 'Sunday': 547,\n",
       " 'able': 548,\n",
       " 'male': 549,\n",
       " 'Hmm,': 550,\n",
       " 'probably': 551,\n",
       " 'leaving': 552,\n",
       " 'Me': 553,\n",
       " 'Which': 554,\n",
       " 'part': 555,\n",
       " 'can?': 556,\n",
       " 'does': 557,\n",
       " 'After': 558,\n",
       " 'Actually': 559,\n",
       " 'things': 560,\n",
       " 'looking': 561,\n",
       " 'Tell': 562,\n",
       " 'camp': 563,\n",
       " 'Only': 564,\n",
       " 'possible.': 565,\n",
       " 'wrong': 566,\n",
       " 'course.': 567,\n",
       " 'shop': 568,\n",
       " 'fun': 569,\n",
       " 'movie': 570,\n",
       " 'asking': 571,\n",
       " 'else': 572,\n",
       " 'phone.': 573,\n",
       " 'feeling': 574,\n",
       " 'Mine': 575,\n",
       " 'Care': 576,\n",
       " 'wish': 577,\n",
       " 'happy': 578,\n",
       " 'there,': 579,\n",
       " 'place.': 580,\n",
       " 'Bugis': 581,\n",
       " 'introduce?': 582,\n",
       " 'day?': 583,\n",
       " 'taking': 584,\n",
       " 'parents': 585,\n",
       " 'Bishan': 586,\n",
       " 'Leona': 587,\n",
       " 'project': 588,\n",
       " 'house.': 589,\n",
       " 'back?': 590,\n",
       " 'Hey!': 591,\n",
       " 'sad.': 592,\n",
       " 'Anyone': 593,\n",
       " 'means': 594,\n",
       " 'name': 595,\n",
       " 'save': 596,\n",
       " 'exams': 597,\n",
       " 'trying': 598,\n",
       " 'thanks': 599,\n",
       " 'photo': 600,\n",
       " 'Who': 601,\n",
       " 'keep': 602,\n",
       " 'months': 603,\n",
       " 'shopping': 604,\n",
       " 'around.': 605,\n",
       " 'Saturday': 606,\n",
       " 'made': 607,\n",
       " 'Ben': 608,\n",
       " 'point': 609,\n",
       " '7': 610,\n",
       " 'blue': 611,\n",
       " \"Haven't\": 612,\n",
       " 'whether': 613,\n",
       " 'year': 614,\n",
       " 'exam': 615,\n",
       " 'know?': 616,\n",
       " '8': 617,\n",
       " 'dear': 618,\n",
       " 'fetch': 619,\n",
       " 'say.': 620,\n",
       " 'New': 621,\n",
       " 'yes,': 622,\n",
       " 'girl.': 623,\n",
       " 'ever': 624,\n",
       " 'found': 625,\n",
       " 'Xin.': 626,\n",
       " 'may': 627,\n",
       " 'miss': 628,\n",
       " 'test': 629,\n",
       " 'do?': 630,\n",
       " 'online.': 631,\n",
       " 'done.': 632,\n",
       " 'reading': 633,\n",
       " 'both': 634,\n",
       " 'actually': 635,\n",
       " 'Huh?': 636,\n",
       " 'asks': 637,\n",
       " 'sit': 638,\n",
       " \"doesn't\": 639,\n",
       " 'Sigh,': 640,\n",
       " 'Get': 641,\n",
       " 'go?': 642,\n",
       " 'service': 643,\n",
       " 'gets': 644,\n",
       " 'want?': 645,\n",
       " 'Must': 646,\n",
       " 'disturb': 647,\n",
       " \"He's\": 648,\n",
       " 'me!': 649,\n",
       " 'ask.': 650,\n",
       " 'month.': 651,\n",
       " 'sleep.': 652,\n",
       " 'reached': 653,\n",
       " 'old': 654,\n",
       " 'real': 655,\n",
       " 'Very': 656,\n",
       " 'morning.': 657,\n",
       " 'beautiful': 658,\n",
       " 'Should': 659,\n",
       " \"we'll\": 660,\n",
       " 'going,': 661,\n",
       " 'changed': 662,\n",
       " 'up?': 663,\n",
       " 'so.': 664,\n",
       " 'year.': 665,\n",
       " 'all.': 666,\n",
       " 'something.': 667,\n",
       " 'friend.': 668,\n",
       " 'OK': 669,\n",
       " 'notes': 670,\n",
       " 'week.': 671,\n",
       " 'have.': 672,\n",
       " 'without': 673,\n",
       " 'kiss': 674,\n",
       " 'Wednesday.': 675,\n",
       " 'man': 676,\n",
       " 'mind,': 677,\n",
       " 'rain': 678,\n",
       " 'evening': 679,\n",
       " 'Study': 680,\n",
       " 'pay': 681,\n",
       " 'woke': 682,\n",
       " 'no.': 683,\n",
       " 'dad': 684,\n",
       " 'Let': 685,\n",
       " 'Li': 686,\n",
       " 'long.': 687,\n",
       " 'night,': 688,\n",
       " 'paid': 689,\n",
       " 'Like': 690,\n",
       " 'yesterday.': 691,\n",
       " 'account': 692,\n",
       " 'past': 693,\n",
       " 'university': 694,\n",
       " 'Love': 695,\n",
       " 'tutorial': 696,\n",
       " 'girls,': 697,\n",
       " 'news.': 698,\n",
       " 'Sorry': 699,\n",
       " 'kids': 700,\n",
       " 'space': 701,\n",
       " 'dream': 702,\n",
       " 'shit': 703,\n",
       " 'started': 704,\n",
       " 'Huh.': 705,\n",
       " 'sure.': 706,\n",
       " 'every': 707,\n",
       " 'thing': 708,\n",
       " 'non': 709,\n",
       " 'far': 710,\n",
       " 'Sure': 711,\n",
       " 'cheese': 712,\n",
       " 'up,': 713,\n",
       " 'dog.': 714,\n",
       " 'Look': 715,\n",
       " 'sound': 716,\n",
       " 'case': 717,\n",
       " 'then,': 718,\n",
       " 'tonight': 719,\n",
       " 'hair.': 720,\n",
       " 'alone.': 721,\n",
       " 'sorry.': 722,\n",
       " 'meet.': 723,\n",
       " 'This': 724,\n",
       " 'times': 725,\n",
       " 'comment': 726,\n",
       " 'choose': 727,\n",
       " 'Science': 728,\n",
       " 'module': 729,\n",
       " 'here,': 730,\n",
       " 'using': 731,\n",
       " '25,': 732,\n",
       " 'Huh,': 733,\n",
       " 'owl': 734,\n",
       " 'Any': 735,\n",
       " 'matter': 736,\n",
       " 'drinks': 737,\n",
       " 'alone?': 738,\n",
       " 'Elaine.': 739,\n",
       " 'slow': 740,\n",
       " 'system': 741,\n",
       " 'bird': 742,\n",
       " 'red': 743,\n",
       " 'fun.': 744,\n",
       " 'boyfriend': 745,\n",
       " 'taxi': 746,\n",
       " 'been?': 747,\n",
       " 'tomorrow,': 748,\n",
       " \"Can't\": 749,\n",
       " 'dear.': 750,\n",
       " 'r': 751,\n",
       " 'ok,': 752,\n",
       " 'wind': 753,\n",
       " 'eating': 754,\n",
       " 'Ok': 755,\n",
       " 'less': 756,\n",
       " 'well.': 757,\n",
       " 'girl,': 758,\n",
       " 'rich': 759,\n",
       " 'used': 760,\n",
       " 'nearer': 761,\n",
       " 'coffee': 762,\n",
       " 'sign': 763,\n",
       " 'rent': 764,\n",
       " 'fire': 765,\n",
       " \"who's\": 766,\n",
       " 'while': 767,\n",
       " 'Too': 768,\n",
       " 'in?': 769,\n",
       " 'slightly': 770,\n",
       " 'Hm.': 771,\n",
       " 'Girl,': 772,\n",
       " 'Think': 773,\n",
       " 'dinner?': 774,\n",
       " 'good,': 775,\n",
       " 'Introduce': 776,\n",
       " 'Haha': 777,\n",
       " 'Same': 778,\n",
       " 'most': 779,\n",
       " 'staying': 780,\n",
       " 'cute': 781,\n",
       " 'room.': 782,\n",
       " 'understand': 783,\n",
       " 'u': 784,\n",
       " 'seat': 785,\n",
       " 'together': 786,\n",
       " 'quiet?': 787,\n",
       " 'can,': 788,\n",
       " 'whole': 789,\n",
       " 'Both': 790,\n",
       " 'want,': 791,\n",
       " 'hour': 792,\n",
       " 'right.': 793,\n",
       " 'reach.': 794,\n",
       " 'need.': 795,\n",
       " 'kind': 796,\n",
       " 'tried': 797,\n",
       " 'stuff.': 798,\n",
       " 'heard': 799,\n",
       " 'celebrate': 800,\n",
       " 'day!': 801,\n",
       " 'sunny': 802,\n",
       " 'interested?': 803,\n",
       " 'fine.': 804,\n",
       " 'first,': 805,\n",
       " 'little': 806,\n",
       " 'Okay': 807,\n",
       " 'cake': 808,\n",
       " 'stupid': 809,\n",
       " 'wear': 810,\n",
       " 'right': 811,\n",
       " 'tired': 812,\n",
       " 'town': 813,\n",
       " '4': 814,\n",
       " 'Damn': 815,\n",
       " 'replied': 816,\n",
       " 'friends.': 817,\n",
       " 'Jos': 818,\n",
       " 'afternoon': 819,\n",
       " 'yours': 820,\n",
       " 'On': 821,\n",
       " 'out?': 822,\n",
       " 'trip': 823,\n",
       " 'There': 824,\n",
       " 'camp.': 825,\n",
       " 'girls': 826,\n",
       " 'cut.': 827,\n",
       " 'no,': 828,\n",
       " 'enough.': 829,\n",
       " 'check': 830,\n",
       " 'first?': 831,\n",
       " '4977236992': 832,\n",
       " '2886': 833,\n",
       " 'teasley': 834,\n",
       " '6499': 835,\n",
       " 'nfd': 836,\n",
       " '5053': 837,\n",
       " 'supervisor': 838,\n",
       " 'applied': 839,\n",
       " 'Shall': 840,\n",
       " '9464': 841,\n",
       " 'woods': 842,\n",
       " 'edge': 843,\n",
       " 'contact': 844,\n",
       " 'allergic': 845,\n",
       " 'bees': 846,\n",
       " 'peanuts': 847,\n",
       " 'body': 848,\n",
       " 'persistent': 849,\n",
       " 'amir': 850,\n",
       " 'le': 851,\n",
       " 'traditional': 852,\n",
       " 'dress': 853,\n",
       " 'Year': 854,\n",
       " 'except': 855,\n",
       " 'decide': 856,\n",
       " 'excited': 857,\n",
       " 'female': 858,\n",
       " '7384391127': 859,\n",
       " '56': 860,\n",
       " 'paper': 861,\n",
       " 'birch': 862,\n",
       " 'biggest': 863,\n",
       " 'hamburger': 864,\n",
       " 'ago.': 865,\n",
       " 'sent': 866,\n",
       " 'hands': 867,\n",
       " 'experience': 868,\n",
       " 'to?': 869,\n",
       " 'beloshveyka.ru105': 870,\n",
       " '4632526021': 871,\n",
       " 'automantenimientosa': 872,\n",
       " 'martin+blando': 873,\n",
       " 'it,': 874,\n",
       " 'injustice': 875,\n",
       " 'committed': 876,\n",
       " 'over.': 877,\n",
       " 'Really': 878,\n",
       " 'waiting': 879,\n",
       " 'profit': 880,\n",
       " 'organization': 881,\n",
       " 'die.': 882,\n",
       " 'text': 883,\n",
       " 'Saturday?': 884,\n",
       " 'Come': 885,\n",
       " 'Toa': 886,\n",
       " 'Payoh': 887,\n",
       " 'televisoresponse': 888,\n",
       " 'What?': 889,\n",
       " 'add': 890,\n",
       " 'sms': 891,\n",
       " 'cake.': 892,\n",
       " 'paper.': 893,\n",
       " 'patio': 894,\n",
       " 'books.': 895,\n",
       " 'working?': 896,\n",
       " 'certuspro.combloqueosjalapa': 897,\n",
       " 'killer': 898,\n",
       " 'bites': 899,\n",
       " 'tired.': 900,\n",
       " 'Some': 901,\n",
       " '+8286698863353': 902,\n",
       " 'Nothing': 903,\n",
       " 'TV': 904,\n",
       " 'Probably': 905,\n",
       " 'table.': 906,\n",
       " 'change.': 907,\n",
       " 'bathroom': 908,\n",
       " 'dye': 909,\n",
       " '35816': 910,\n",
       " 'webwood': 911,\n",
       " 'union': 912,\n",
       " 'darling': 913,\n",
       " 'Really.': 914,\n",
       " 'us.': 915,\n",
       " 'line.': 916,\n",
       " 'home?': 917,\n",
       " 'prefer': 918,\n",
       " 'now!': 919,\n",
       " 'Oops.': 920,\n",
       " 'kkaicd1.pixnet.net': 921,\n",
       " 'medieval': 922,\n",
       " 'offensive': 923,\n",
       " 'Sharis,': 924,\n",
       " 'night!': 925,\n",
       " 'Chinese.': 926,\n",
       " '963': 927,\n",
       " 'crested': 928,\n",
       " 'ends': 929,\n",
       " 'bunkerbranch.tumblr.com': 930,\n",
       " '4127407457': 931,\n",
       " 'selling': 932,\n",
       " 'important': 933,\n",
       " 'music': 934,\n",
       " 'meeting.': 935,\n",
       " 'accept': 936,\n",
       " 'personal': 937,\n",
       " 'checks': 938,\n",
       " 'dance.': 939,\n",
       " '2901092582': 940,\n",
       " 'buying': 941,\n",
       " '9186054830': 942,\n",
       " 'Chinese,': 943,\n",
       " 'postal': 944,\n",
       " 'taxation': 945,\n",
       " 'www.sudinfo.be': 946,\n",
       " 'worm': 947,\n",
       " 'discuss': 948,\n",
       " 'weeks.': 949,\n",
       " 'photos.': 950,\n",
       " 'keeps': 951,\n",
       " 'www.tudecora.com': 952,\n",
       " 'meter': 953,\n",
       " '10': 954,\n",
       " 'along?': 955,\n",
       " 'bored': 956,\n",
       " \"friend's\": 957,\n",
       " 'Try': 958,\n",
       " 'library': 959,\n",
       " 'closed': 960,\n",
       " '+3515218895': 961,\n",
       " 'making': 962,\n",
       " 'Thanks!': 963,\n",
       " 'gandchudaihardcor.html': 964,\n",
       " '102': 965,\n",
       " 'flagstad': 966,\n",
       " 'Thought': 967,\n",
       " 'receive': 968,\n",
       " 'message?': 969,\n",
       " '8260': 970,\n",
       " 'john': 971,\n",
       " 'bowdoin': 972,\n",
       " 'myanmarsocialonlinemedia.com': 973,\n",
       " 'prevailing': 974,\n",
       " 'later,': 975,\n",
       " 'break': 976,\n",
       " 'sending': 977,\n",
       " 'Laugh,': 978,\n",
       " 'half': 979,\n",
       " 'drugs': 980,\n",
       " 'avoided': 981,\n",
       " 'jobs': 982,\n",
       " 'pay.': 983,\n",
       " '7870': 984,\n",
       " 'preston': 985,\n",
       " 'stand': 986,\n",
       " 'since': 987,\n",
       " 'www.boehringeringelheim.com': 988,\n",
       " 'darling,': 989,\n",
       " 'Had': 990,\n",
       " 'break.': 991,\n",
       " '5016': 992,\n",
       " 'paquin': 993,\n",
       " 'street': 994,\n",
       " 'reallyloud.co.uksimaii': 995,\n",
       " 'paintings': 996,\n",
       " 'gallery': 997,\n",
       " 'www.twitchquotes.com': 998,\n",
       " 'free,': 999,\n",
       " 'goes': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tokenizer_e.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8632d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index=loaded_tokenizer.word_index.keys()\n",
    "tokenizer_e.word_index=loaded_tokenizer_e.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c6848591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense,Input,GRU,Embedding,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97aca652",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab_size=7815\n",
    "decoder_vocab_size=5464\n",
    "lstm_size = 100\n",
    "embedding_dim =128\n",
    "att_units = 100\n",
    "dec_units=100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28cac2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = encoder_decoder(encoder_vocab_size,decoder_vocab_size,39,lstm_size,embedding_dim,att_units, 40,'concat',dec_units,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b00c3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.encoder_decoder at 0x285864af0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7dc86a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_encoder_input = tf.constant([[1, 2, 3, 4, 5]])  # Replace with your own input\n",
    "dummy_decoder_input = tf.constant([[6, 7, 8, 9, 10]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3ae40575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 5465), dtype=float32, numpy=\n",
       "array([[[-2.2220929e-04,  9.5734210e-04,  1.8716115e-03, ...,\n",
       "         -8.7533968e-05, -3.0937325e-03,  3.0638601e-04],\n",
       "        [-3.4844616e-04, -2.8433601e-04,  2.7361875e-03, ...,\n",
       "         -1.9186018e-03, -2.4391573e-03,  2.0527642e-03],\n",
       "        [ 8.2773832e-04, -3.2353846e-04,  2.5894430e-03, ...,\n",
       "         -2.1019522e-03, -1.6265627e-03,  2.2080115e-03],\n",
       "        [-3.5861123e-04,  5.5544788e-04,  1.9033194e-03, ...,\n",
       "         -5.2289100e-04, -1.5952929e-03, -9.3154743e-04],\n",
       "        [-1.1149305e-03, -4.8895524e-04,  1.7227859e-03, ...,\n",
       "         -1.2337214e-03, -1.9210590e-03, -1.6469908e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model([dummy_encoder_input, dummy_decoder_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721e6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50728747",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights('/Users/sayedraheel/Desktop/NLP/Sentence_correction/encoder_decoder_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb42e1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <__main__.encoder_decoder object at 0x285864af0>>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51e45805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "def evaluate(sentence):\n",
    "  max_length_targ = 40\n",
    "  max_length_inp  = 39\n",
    "  \n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "  # sentence = preprocess(sentence)\n",
    "  # sentence = sentence.strip()\n",
    "  # print(sentence)\n",
    "  #pdb.set_trace()\n",
    "  inputs = [] \n",
    "  # for word in sentence.split():\n",
    "  #     print(word)\n",
    "  #     inputs.append(tokenizer.word_index[word])\n",
    "  inputs = tokenizer.texts_to_sequences([sentence])\n",
    "  inputs =  tf.keras.preprocessing.sequence.pad_sequences(inputs,maxlen=max_length_inp,padding='post') \n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "  result = '' \n",
    "\n",
    "  initial_state=model2.layers[0].initialize_states(batch_size=1)\n",
    "  encoder_outputs, state_h,state_c = model2.layers[0](inputs,initial_state)   \n",
    "  dec_input = tf.expand_dims([tokenizer_e.word_index['<start>']], 0)\n",
    "  # dec_input=tf.expand_dims([tokenizer_e.word_index['\\t']],0)\n",
    "\n",
    "\n",
    "  for i in range(max_length_targ):\n",
    "    Output,state_h,state_c,att_weights,_ = model2.layers[1].oneStepDecoder(dec_input,encoder_outputs,state_h,state_c)\n",
    "    #Beam Search Decoder\n",
    "    Result_beam_list=beam_search_decoder(Output,k=1)\n",
    "    Result_beam=Result_beam_list[0][0]\n",
    "\n",
    "    attention_weights = tf.reshape(att_weights, (-1, ))\n",
    "    attention_plot[i]  = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(Output[0]).numpy()\n",
    "  \n",
    "    result += tokenizer_e.index_word[Result_beam[0]] + ' '\n",
    "    if tokenizer_e.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e96b4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(input_sentence):\n",
    "\n",
    "  '''\n",
    "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "  C. Initialize index of <\\t> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "  D. till we reach max_length of decoder or till the model predicted character <\\n>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         And get the character using the tokenizer(character index) and then store it in a string.\n",
    "  E. Return the predicted sentence\n",
    "\n",
    "  '''\n",
    "\n",
    "\n",
    "  input_sequence=tokenizer.texts_to_sequences([input_sentence])\n",
    "  \n",
    "\n",
    "  inputs=pad_sequences(input_sequence,maxlen=39,padding='post')\n",
    "  inputs=tf.convert_to_tensor(inputs)\n",
    "  result=''\n",
    "  units=100\n",
    "  hidden=[tf.zeros((1,units))]\n",
    "  encoder_output,hidden_state,cell_state=model2.layers[0](inputs,hidden)\n",
    "  dec_hidden=hidden_state\n",
    "  dec_input=tf.expand_dims([tokenizer_e.word_index['<start>']],0)\n",
    "  for t in range(40):\n",
    "      predictions,dec_hidden,cell_state,attention_weights,context_vector=model2.layers[1].oneStepDecoder(dec_input,encoder_output,dec_hidden,cell_state)\n",
    "\n",
    "      predicted_id=tf.argmax(predictions[0]).numpy()\n",
    "      result+=tokenizer_e.index_word[predicted_id] + ' '\n",
    "      if tokenizer_e.word_index['<end>']==predicted_id:\n",
    "          return result\n",
    "      dec_input= tf.expand_dims([predicted_id],0)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00368780",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \" hi am onkar iz tryinggg to \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06bd386a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict(\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m[index])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "predict(text.values[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c3bf286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "90a353a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour_input_text\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Translate and evaluate the input text\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m translation_result, original_sentence, attention_plot \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Print the translation result and attention plot\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Sentence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, original_sentence)\n",
      "Cell \u001b[0;32mIn[84], line 10\u001b[0m, in \u001b[0;36mtranslate_and_evaluate\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(inputs)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Translate the input text using the predict function\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m translation \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluate the translation using the evaluate function\u001b[39;00m\n\u001b[1;32m     13\u001b[0m translation_result, original_sentence, attention_plot \u001b[38;5;241m=\u001b[39m evaluate(translation)\n",
      "Cell \u001b[0;32mIn[66], line 15\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(input_sentence):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m  '''\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m   input_sequence\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts_to_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_sentence\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m   inputs\u001b[38;5;241m=\u001b[39mpad_sequences(input_sequence,maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m39\u001b[39m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m   inputs\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(inputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/keras/src/preprocessing/text.py:357\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtexts_to_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts):\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transforms each text in texts to a sequence of integers.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Only top `num_words-1` most frequent words will be taken into account.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m        A list of sequences.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts_to_sequences_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/keras/src/preprocessing/text.py:386\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m         seq \u001b[38;5;241m=\u001b[39m \u001b[43mtext_to_word_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer(text)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/keras/src/preprocessing/text.py:74\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a text to a sequence of words (or tokens).\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mDeprecated: `tf.keras.preprocessing.text.text_to_word_sequence` does not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    A list of words (or tokens).\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 74\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[43minput_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     76\u001b[0m translate_dict \u001b[38;5;241m=\u001b[39m {c: split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m filters}\n\u001b[1;32m     77\u001b[0m translate_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(translate_dict)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:430\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    422\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    423\u001b[0m   \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m    424\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def translate_and_evaluate(input_text):\n",
    "    # Tokenize the input text\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(input_sequence, maxlen=39, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    # Translate the input text using the predict function\n",
    "    translation = predict(inputs)\n",
    "\n",
    "    # Evaluate the translation using the evaluate function\n",
    "    translation_result, original_sentence, attention_plot = evaluate(translation)\n",
    "    \n",
    "    return translation_result, original_sentence, attention_plot\n",
    "\n",
    "# Replace 'your_input_text' with the actual text you want to translate and evaluate\n",
    "input_text = \"your_input_text\"\n",
    "\n",
    "# Translate and evaluate the input text\n",
    "translation_result, original_sentence, attention_plot = translate_and_evaluate(input_text)\n",
    "\n",
    "# Print the translation result and attention plot\n",
    "print(\"Original Sentence:\", original_sentence)\n",
    "print(\"Translation Result:\", translation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ee857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
